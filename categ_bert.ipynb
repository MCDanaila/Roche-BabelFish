{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Bry9MfTxKy9R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from pandas import read_excel\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "mASft1i5Ky9R",
        "outputId": "b8dca91a-8057-445b-9342-de033a989e60"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-aaa97032-7a8d-4cbf-b1b4-693ece79d15a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LOINCID</th>\n",
              "      <th>Publication Version ID</th>\n",
              "      <th>Model</th>\n",
              "      <th>Vendor Analyte Code</th>\n",
              "      <th>Vendor Analyte Name</th>\n",
              "      <th>Vendor Specimen Description</th>\n",
              "      <th>Vendor Result Description</th>\n",
              "      <th>LOINC Term</th>\n",
              "      <th>LOINC Long Name</th>\n",
              "      <th>Component</th>\n",
              "      <th>System</th>\n",
              "      <th>LOINC Version ID</th>\n",
              "      <th>Unit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>RMD_v1.0</td>\n",
              "      <td>cobas 6800</td>\n",
              "      <td>89342-0^Babesia^LN</td>\n",
              "      <td>Babesia qual. 480 Tests cobas 6800/8800 IVD</td>\n",
              "      <td>whole blood</td>\n",
              "      <td>Ord</td>\n",
              "      <td>89342-0</td>\n",
              "      <td>Babesia sp 18S rRNA [Presence] in Blood by NAA...</td>\n",
              "      <td>Babesia sp 18S rRNA</td>\n",
              "      <td>Bld</td>\n",
              "      <td>2.68</td>\n",
              "      <td>Ord</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "      <td>RMD_v1.0</td>\n",
              "      <td>cobas 8800</td>\n",
              "      <td>89342-0^Babesia^LN</td>\n",
              "      <td>Babesia qual. 480 Tests cobas 6800/8800 IVD</td>\n",
              "      <td>whole blood</td>\n",
              "      <td>Ord</td>\n",
              "      <td>89342-0</td>\n",
              "      <td>Babesia sp 18S rRNA [Presence] in Blood by NAA...</td>\n",
              "      <td>Babesia sp 18S rRNA</td>\n",
              "      <td>Bld</td>\n",
              "      <td>2.68</td>\n",
              "      <td>Ord</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aaa97032-7a8d-4cbf-b1b4-693ece79d15a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aaa97032-7a8d-4cbf-b1b4-693ece79d15a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aaa97032-7a8d-4cbf-b1b4-693ece79d15a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   LOINCID Publication Version ID       Model Vendor Analyte Code  \\\n",
              "0       30               RMD_v1.0  cobas 6800  89342-0^Babesia^LN   \n",
              "1       40               RMD_v1.0  cobas 8800  89342-0^Babesia^LN   \n",
              "\n",
              "                           Vendor Analyte Name Vendor Specimen Description  \\\n",
              "0  Babesia qual. 480 Tests cobas 6800/8800 IVD                 whole blood   \n",
              "1  Babesia qual. 480 Tests cobas 6800/8800 IVD                 whole blood   \n",
              "\n",
              "  Vendor Result Description LOINC Term  \\\n",
              "0                       Ord    89342-0   \n",
              "1                       Ord    89342-0   \n",
              "\n",
              "                                     LOINC Long Name            Component  \\\n",
              "0  Babesia sp 18S rRNA [Presence] in Blood by NAA...  Babesia sp 18S rRNA   \n",
              "1  Babesia sp 18S rRNA [Presence] in Blood by NAA...  Babesia sp 18S rRNA   \n",
              "\n",
              "  System LOINC Version ID Unit  \n",
              "0    Bld             2.68  Ord  \n",
              "1    Bld             2.68  Ord  "
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_mapping = 'ZHAW_DATA/Mapping_table_hospital_KIS_LIS.xlsx' \n",
        "file_roche_LOINC_db = 'Roche_LOINC_Database_filter.xlsm'\n",
        "file_lab_codes = 'ZHAW_DATA/Example_Data_Laboratory_Codes_eng.xlsx'\n",
        "\n",
        "df = read_excel(file_roche_LOINC_db, engine='openpyxl')\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPtBzvq_Ky9S",
        "outputId": "b240c00b-e482-4d29-d35c-d1edff12a170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  LOINC Term                                    LOINC Long Name       Model\n",
            "0    89342-0  Babesia sp 18S rRNA [Presence] in Blood by NAA...  cobas 6800\n",
            "1    89342-0  Babesia sp 18S rRNA [Presence] in Blood by NAA...  cobas 8800\n",
            "LOINC Term         object\n",
            "LOINC Long Name    object\n",
            "Model              object\n",
            "dtype: object\n",
            "  LOINC Term                                    LOINC Long Name       Model  \\\n",
            "0    89342-0  Babesia sp 18S rRNA [Presence] in Blood by NAA...  cobas 6800   \n",
            "1    89342-0  Babesia sp 18S rRNA [Presence] in Blood by NAA...  cobas 8800   \n",
            "\n",
            "   LOINC Code LN  \n",
            "0             66  \n",
            "1             66  \n",
            "LOINC Term           string\n",
            "LOINC Long Name    category\n",
            "Model                string\n",
            "LOINC Code LN         int16\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "data_df = df.loc[:,('LOINC Term', 'LOINC Long Name', 'Model')]\n",
        "print(data_df.head(2))\n",
        "print(data_df.dtypes)\n",
        "# First, change the type of the specified columns to 'category'. This will \n",
        "# assign a 'code' to each unique category value.\n",
        "data_df.loc[:,'LOINC Term'] = data_df.loc[:,'LOINC Term'].astype('string')\n",
        "data_df.loc[:,'Model'] = data_df.loc[:,'Model'].astype('string')\n",
        "data_df.loc[:,'LOINC Long Name'] = data_df.loc[:,'LOINC Long Name'].astype('category')\n",
        "\n",
        "# Second, replace the strings with their code values.\n",
        "data_df.loc[:,'LOINC Code LN'] = data_df.loc[:,'LOINC Long Name'].cat.codes\n",
        "\n",
        "# Display the table--notice how the above columns are all integers now.\n",
        "print(data_df.head(2))\n",
        "print(data_df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaHGXRmJKy9S",
        "outputId": "94d219da-78f9-495a-d054-b2948b863779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Training size: 5,604\n",
            "Validation size: 700\n",
            "      Test size: 701\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# First, calculate the split sizes. 80% training, 10% validation, 10% test.\n",
        "train_size = int(0.8 * len(data_df))\n",
        "val_size = int(0.1 * len(data_df))\n",
        "test_size = len(data_df) - (train_size + val_size)\n",
        "\n",
        "# Sanity check the sizes.\n",
        "assert((train_size + val_size + test_size) == len(data_df))\n",
        "\n",
        "# Create a list of indeces for all of the samples in the dataset.\n",
        "indeces = np.arange(0, len(data_df))\n",
        "\n",
        "# Shuffle the indeces randomly.\n",
        "random.shuffle(indeces)\n",
        "\n",
        "# Get a list of indeces for each of the splits.\n",
        "train_idx = indeces[0:train_size]\n",
        "val_idx = indeces[train_size:(train_size + val_size)]\n",
        "test_idx = indeces[(train_size + val_size):]\n",
        "\n",
        "# Sanity check\n",
        "assert(len(train_idx) == train_size)\n",
        "assert(len(test_idx) == test_size)\n",
        "\n",
        "# With these lists, we can now select the corresponding dataframe rows using, \n",
        "# e.g., train_df = data_df.iloc[train_idx] \n",
        "\n",
        "print('  Training size: {:,}'.format(train_size))\n",
        "print('Validation size: {:,}'.format(val_size))\n",
        "print('      Test size: {:,}'.format(test_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TPnbl0KKy9S",
        "outputId": "bd81d928-fd32-44d2-e6ad-93440dca44cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  DONE.\n",
            "Dataset contains 7,005 samples.\n"
          ]
        }
      ],
      "source": [
        "# This will hold all of the dataset samples, as strings.\n",
        "sen_w_feats = []\n",
        "\n",
        "# The labels for the samples.\n",
        "labels = []\n",
        "\n",
        "# For each of the samples...\n",
        "for index, row in data_df.iterrows():\n",
        "\n",
        "    # Piece it together...    \n",
        "    combined = \"\"\n",
        "    combined += \"The Model of the machine used is {:}, \".format(row['Model'])\n",
        "    #combined += \"I am {:} years old. \".format(row[\"Age\"])\n",
        "    #combined += \"I rate this item {:} out of 5 stars. \".format(row[\"Rating\"])\n",
        "\n",
        "    # Finally, append the review the text!\n",
        "    combined += row['LOINC Long Name']\n",
        "    \n",
        "    # Add the combined text to the list.\n",
        "    sen_w_feats.append(combined)\n",
        "\n",
        "    # Also record the sample's label.\n",
        "    labels.append(row['LOINC Code LN'])\n",
        "\n",
        "print('  DONE.')\n",
        "\n",
        "print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXRqTxQjKy9T",
        "outputId": "10f8c1c0-b72f-4240-da9a-e0fd86ab6dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnYiCT9EKy9T",
        "outputId": "65f550a1-e6e5-4339-81ec-3e3c7629969c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers\n",
            "  Using cached transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp39-cp39-macosx_10_9_x86_64.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.6/197.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2022.9.13-cp39-cp39-macosx_10_9_x86_64.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.9/293.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Using cached huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp39-cp39-macosx_10_11_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
            "  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (1.26.10)\n",
            "Installing collected packages: tokenizers, typing-extensions, regex, pyyaml, filelock, huggingface-hub, transformers\n",
            "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed filelock-3.8.0 huggingface-hub-0.10.0 pyyaml-6.0 regex-2022.9.13 tokenizers-0.12.1 transformers-4.22.2 typing-extensions-4.3.0\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mLoading BERT tokenizer...\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "ckiVrKyDKy9T",
        "outputId": "971de037-e4b2-4301-b8db-8f3b293cea8e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0bb44be6-ca95-468c-a75e-d24a723e1936\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">LOINC Term</th>\n",
              "      <th colspan=\"4\" halign=\"left\">LOINC Long Name</th>\n",
              "      <th colspan=\"4\" halign=\"left\">Model</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOINC Code LN</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>93495-0</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>2-Ethylidene-1,5-Dimethyl-3,3-Diphenylpyrrolid...</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas c 501</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>77752-4</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>2-Ethylidene-1,5-Dimethyl-3,3-Diphenylpyrrolid...</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas c 501</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>83070-3</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>25-Hydroxyvitamin D3+25-Hydroxyvitamin D2 [Mas...</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas e 411</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>83071-1</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>25-Hydroxyvitamin D3+25-Hydroxyvitamin D2 [Mol...</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas e 411</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>19321-9</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6-Monoacetylmorphine (6-MAM) [Presence] in Uri...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>cobas c 501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4017-0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>quiNIDine [Mass/volume] in Serum or Plasma --t...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas c 303</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>14899-9</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>quiNIDine [Moles/volume] in Serum or Plasma</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas c 303</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>97196-0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>quiNIDine [Moles/volume] in Serum or Plasma --...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas c 303</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>97197-8</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>quiNIDine [Moles/volume] in Serum or Plasma --...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>cobas c 303</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>68324-3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>von Willebrand factor (vWf).activity actual/no...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>cobas t 511</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>511 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bb44be6-ca95-468c-a75e-d24a723e1936')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0bb44be6-ca95-468c-a75e-d24a723e1936 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0bb44be6-ca95-468c-a75e-d24a723e1936');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              LOINC Term                      LOINC Long Name         \\\n",
              "                   count unique      top freq           count unique   \n",
              "LOINC Code LN                                                          \n",
              "0                     17      1  93495-0   17              17      1   \n",
              "1                     16      1  77752-4   16              16      1   \n",
              "2                     36      1  83070-3   36              36      1   \n",
              "3                     36      1  83071-1   36              36      1   \n",
              "4                      3      1  19321-9    3               3      1   \n",
              "...                  ...    ...      ...  ...             ...    ...   \n",
              "506                    6      1   4017-0    6               6      1   \n",
              "507                    6      1  14899-9    6               6      1   \n",
              "508                    6      1  97196-0    6               6      1   \n",
              "509                    6      1  97197-8    6               6      1   \n",
              "510                    2      1  68324-3    2               2      1   \n",
              "\n",
              "                                                                      Model  \\\n",
              "                                                             top freq count   \n",
              "LOINC Code LN                                                                 \n",
              "0              2-Ethylidene-1,5-Dimethyl-3,3-Diphenylpyrrolid...   17    17   \n",
              "1              2-Ethylidene-1,5-Dimethyl-3,3-Diphenylpyrrolid...   16    16   \n",
              "2              25-Hydroxyvitamin D3+25-Hydroxyvitamin D2 [Mas...   36    36   \n",
              "3              25-Hydroxyvitamin D3+25-Hydroxyvitamin D2 [Mol...   36    36   \n",
              "4              6-Monoacetylmorphine (6-MAM) [Presence] in Uri...    3     3   \n",
              "...                                                          ...  ...   ...   \n",
              "506            quiNIDine [Mass/volume] in Serum or Plasma --t...    6     6   \n",
              "507                  quiNIDine [Moles/volume] in Serum or Plasma    6     6   \n",
              "508            quiNIDine [Moles/volume] in Serum or Plasma --...    6     6   \n",
              "509            quiNIDine [Moles/volume] in Serum or Plasma --...    6     6   \n",
              "510            von Willebrand factor (vWf).activity actual/no...    2     2   \n",
              "\n",
              "                                        \n",
              "              unique          top freq  \n",
              "LOINC Code LN                           \n",
              "0                  6  cobas c 501    4  \n",
              "1                  6  cobas c 501    4  \n",
              "2                  6  cobas e 411    8  \n",
              "3                  6  cobas e 411    8  \n",
              "4                  3  cobas c 501    1  \n",
              "...              ...          ...  ...  \n",
              "506                6  cobas c 303    1  \n",
              "507                6  cobas c 303    1  \n",
              "508                6  cobas c 303    1  \n",
              "509                6  cobas c 303    1  \n",
              "510                2  cobas t 511    1  \n",
              "\n",
              "[511 rows x 12 columns]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.groupby('LOINC Code LN').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CYEg52HKy9U",
        "outputId": "a43408cc-4d49-4f88-bfed-f89ddf03797c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=511, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 511, # The number of output labels--2 for binary classification.\n",
        ")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "v6DLBzvsKy9U"
      },
      "outputs": [],
      "source": [
        "# Larger batch sizes tend to be better, and we can fit this in memory.\n",
        "batch_size = 32\n",
        "\n",
        "# I used a smaller learning rate to combat over-fitting that I was seeing in the\n",
        "# validation loss. I could probably try even smaller.\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# Number of training epochs. \n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbPksH6-Ky9U",
        "outputId": "d27e2557-d005-4afa-938c-62b8cb96cc01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  62\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sen_w_feats:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qLX2YicKy9V",
        "outputId": "e95fb912-bd2d-42cb-930a-80950de7f0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding all reviews in the dataset...\n",
            "torch.Size([7005, 62])\n",
            "torch.Size([7005, 62])\n",
            "torch.Size([7005])\n",
            "DONE.\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "print('Encoding all reviews in the dataset...')\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sen_w_feats:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "#print(labels)\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print(input_ids.shape)\n",
        "print(attention_masks.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Wm_KGxgDKy9V"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Split the samples, and create TensorDatasets for each split. \n",
        "train_dataset = TensorDataset(input_ids[train_idx], attention_masks[train_idx], labels[train_idx])\n",
        "val_dataset = TensorDataset(input_ids[val_idx], attention_masks[val_idx], labels[val_idx])\n",
        "test_dataset = TensorDataset(input_ids[test_idx], attention_masks[test_idx], labels[test_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "6HgDpqAUKy9V"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAxzjUamKy9V",
        "outputId": "30ca7fe1-dbb2-44b5-9827-afc7320c8ce0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, \n",
        "                  eps = 1e-8 \n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Tece4JlvKy9W"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples!)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "TqOsuM9lKy9W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4UAj0P9Ky9W",
        "outputId": "3d7aa871-a0b4-4873-e4dc-e2ec1241588e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:38.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 6.06\n",
            "  Training epcoh took: 0:00:55\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.13\n",
            "  Validation Loss: 5.71\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:26.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:39.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 5.56\n",
            "  Training epcoh took: 0:00:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.22\n",
            "  Validation Loss: 5.23\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:26.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:53.\n",
            "\n",
            "  Average training loss: 5.15\n",
            "  Training epcoh took: 0:00:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.29\n",
            "  Validation Loss: 4.83\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:53.\n",
            "\n",
            "  Average training loss: 4.77\n",
            "  Training epcoh took: 0:00:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.36\n",
            "  Validation Loss: 4.46\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 5 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 4.42\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.42\n",
            "  Validation Loss: 4.14\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 6 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 4.13\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.44\n",
            "  Validation Loss: 3.84\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 7 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 3.85\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 3.58\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 8 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 3.60\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 3.33\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 9 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 3.36\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 3.11\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 10 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 3.15\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 2.90\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 11 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 2.94\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 2.72\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 12 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 2.77\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 2.54\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 13 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 2.60\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  Validation Loss: 2.39\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 14 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 2.43\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 2.24\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 15 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 2.29\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 2.11\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 16 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 2.15\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 1.99\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 17 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 2.03\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 1.87\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 18 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.91\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 1.76\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 19 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation Loss: 1.66\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 20 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.70\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 1.58\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 21 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.61\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 22 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.52\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 1.41\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 23 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.45\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation Loss: 1.35\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 24 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.37\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 1.28\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 25 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.30\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 1.22\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 26 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 1.16\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 27 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 1.18\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 1.11\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 28 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 1.13\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "  Validation Loss: 1.07\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 29 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.08\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation Loss: 1.02\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 30 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 31 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.94\n",
            "  Validation Loss: 0.94\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 32 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation Loss: 0.91\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 33 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.92\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation Loss: 0.88\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 34 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.89\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation Loss: 0.85\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 35 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.86\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.82\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 36 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.84\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.80\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 37 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.78\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 38 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.76\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 39 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.77\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 40 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.75\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 41 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.73\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 42 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 43 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 44 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:55.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:01:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 45 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.67\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 46 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 47 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 48 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 49 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 50 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of    176.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    176.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    176.    Elapsed: 0:00:41.\n",
            "  Batch   160  of    176.    Elapsed: 0:00:54.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:00:59\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:51:29 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "10lVx_BqOy6-",
        "outputId": "324ae3fb-9fb9-4e17-e33d-0a0cdd78ca9f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-05ef23fc-ee0b-4d66-b32e-4c63a407edef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.06</td>\n",
              "      <td>5.71</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0:00:55</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.56</td>\n",
              "      <td>5.23</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0:00:56</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.15</td>\n",
              "      <td>4.83</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0:00:58</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.77</td>\n",
              "      <td>4.46</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0:00:58</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.42</td>\n",
              "      <td>4.14</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.13</td>\n",
              "      <td>3.84</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.85</td>\n",
              "      <td>3.58</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3.60</td>\n",
              "      <td>3.33</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3.36</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3.15</td>\n",
              "      <td>2.90</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2.94</td>\n",
              "      <td>2.72</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.77</td>\n",
              "      <td>2.54</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.60</td>\n",
              "      <td>2.39</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2.43</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2.29</td>\n",
              "      <td>2.11</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2.15</td>\n",
              "      <td>1.99</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2.03</td>\n",
              "      <td>1.87</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.91</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.80</td>\n",
              "      <td>1.66</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.70</td>\n",
              "      <td>1.58</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.61</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.52</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.45</td>\n",
              "      <td>1.35</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.37</td>\n",
              "      <td>1.28</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.30</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.24</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.18</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.13</td>\n",
              "      <td>1.07</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.08</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.04</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.99</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.86</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.79</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.72</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.69</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:01:00</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.66</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.66</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0:00:59</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05ef23fc-ee0b-4d66-b32e-4c63a407edef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05ef23fc-ee0b-4d66-b32e-4c63a407edef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05ef23fc-ee0b-4d66-b32e-4c63a407edef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               6.06         5.71           0.13       0:00:55         0:00:02\n",
              "2               5.56         5.23           0.22       0:00:56         0:00:02\n",
              "3               5.15         4.83           0.29       0:00:58         0:00:03\n",
              "4               4.77         4.46           0.36       0:00:58         0:00:03\n",
              "5               4.42         4.14           0.42       0:00:59         0:00:03\n",
              "6               4.13         3.84           0.44       0:00:59         0:00:03\n",
              "7               3.85         3.58           0.52       0:00:59         0:00:03\n",
              "8               3.60         3.33           0.55       0:00:59         0:00:03\n",
              "9               3.36         3.11           0.60       0:01:00         0:00:03\n",
              "10              3.15         2.90           0.63       0:01:00         0:00:03\n",
              "11              2.94         2.72           0.68       0:01:00         0:00:03\n",
              "12              2.77         2.54           0.72       0:01:00         0:00:03\n",
              "13              2.60         2.39           0.75       0:00:59         0:00:03\n",
              "14              2.43         2.24           0.77       0:00:59         0:00:03\n",
              "15              2.29         2.11           0.78       0:00:59         0:00:03\n",
              "16              2.15         1.99           0.81       0:00:59         0:00:03\n",
              "17              2.03         1.87           0.84       0:00:59         0:00:03\n",
              "18              1.91         1.76           0.86       0:00:59         0:00:03\n",
              "19              1.80         1.66           0.87       0:00:59         0:00:03\n",
              "20              1.70         1.58           0.88       0:00:59         0:00:03\n",
              "21              1.61         1.49           0.89       0:00:59         0:00:03\n",
              "22              1.52         1.41           0.90       0:00:59         0:00:03\n",
              "23              1.45         1.35           0.89       0:00:59         0:00:03\n",
              "24              1.37         1.28           0.91       0:00:59         0:00:03\n",
              "25              1.30         1.22           0.91       0:01:00         0:00:03\n",
              "26              1.24         1.16           0.92       0:01:00         0:00:03\n",
              "27              1.18         1.11           0.92       0:01:00         0:00:03\n",
              "28              1.13         1.07           0.93       0:01:00         0:00:03\n",
              "29              1.08         1.02           0.94       0:01:00         0:00:03\n",
              "30              1.04         0.98           0.94       0:00:59         0:00:03\n",
              "31              0.99         0.94           0.94       0:00:59         0:00:03\n",
              "32              0.96         0.91           0.95       0:00:59         0:00:03\n",
              "33              0.92         0.88           0.95       0:00:59         0:00:03\n",
              "34              0.89         0.85           0.95       0:00:59         0:00:03\n",
              "35              0.86         0.82           0.96       0:00:59         0:00:03\n",
              "36              0.84         0.80           0.96       0:00:59         0:00:03\n",
              "37              0.81         0.78           0.96       0:00:59         0:00:03\n",
              "38              0.79         0.76           0.96       0:00:59         0:00:03\n",
              "39              0.77         0.74           0.96       0:00:59         0:00:03\n",
              "40              0.75         0.72           0.96       0:00:59         0:00:03\n",
              "41              0.73         0.71           0.97       0:00:59         0:00:03\n",
              "42              0.72         0.70           0.97       0:01:00         0:00:03\n",
              "43              0.70         0.69           0.97       0:01:00         0:00:03\n",
              "44              0.69         0.68           0.97       0:01:00         0:00:03\n",
              "45              0.68         0.67           0.97       0:00:59         0:00:03\n",
              "46              0.68         0.66           0.97       0:00:59         0:00:03\n",
              "47              0.67         0.66           0.97       0:00:59         0:00:03\n",
              "48              0.67         0.65           0.97       0:00:59         0:00:03\n",
              "49              0.66         0.65           0.97       0:00:59         0:00:03\n",
              "50              0.66         0.65           0.97       0:00:59         0:00:03"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap (doesn't seem to work in Colab).\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "NJybgjOsO6Of",
        "outputId": "7a9fb003-4d40-4f55-a385-ba9fd35ef5a0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_lab_codes = 'ZHAW_DATA/Example_Data_Laboratory_Codes_eng.xlsx'\n",
        "\n",
        "df_new_lab_codes = read_excel(file_lab_codes, engine='openpyxl')\n",
        "print(df_new_lab_codes.head(2)) # shows headers with top 5 rows\n",
        "\n",
        "new_data_df = df_new_lab_codes.loc[:,('Parameter', 'Long name', 'Device')]\n",
        "print(new_data_df.dtypes)\n",
        "# First, change the type of the specified columns to 'category'. This will \n",
        "# assign a 'code' to each unique category value.\n",
        "new_data_df.loc[:,'Parameter'] = new_data_df.loc[:,'Parameter'].astype('string')\n",
        "new_data_df.loc[:,'Device'] = new_data_df.loc[:,'Device'].astype('string')\n",
        "new_data_df.loc[:,'Long name'] = new_data_df.loc[:,'Long name'].astype('category')\n",
        "\n",
        "# Second, replace the strings with their code values.\n",
        "new_data_df.loc[:,'Code LN'] = new_data_df.loc[:,'Long name'].cat.codes\n",
        "\n",
        "# Display the table--notice how the above columns are all integers now.\n",
        "print(new_data_df.head(2))\n",
        "print(new_data_df.dtypes)\n",
        "\n",
        "new_data_size = len(new_data_df)\n",
        "\n",
        "# Create a list of indeces for all of the samples in the dataset.\n",
        "new_test_idx = np.arange(0, len(new_data_df))\n",
        "\n",
        "# Sanity check\n",
        "assert(len(new_test_idx) == new_data_size)\n",
        "\n",
        "print('New Test size: {:,}'.format(new_data_size))\n",
        "\n",
        "# This will hold all of the dataset samples, as strings.\n",
        "ln_w_dev = []\n",
        "\n",
        "# The labels for the samples.\n",
        "new_labels = []\n",
        "\n",
        "# For each of the samples...\n",
        "for index, row in new_data_df.iterrows():\n",
        "\n",
        "    # Piece it together...    \n",
        "    combined = \"\"\n",
        "    combined += \"The Model of the machine used is {:}, \".format(row['Device'])\n",
        "    #combined += \"I am {:} years old. \".format(row[\"Age\"])\n",
        "    #combined += \"I rate this item {:} out of 5 stars. \".format(row[\"Rating\"])\n",
        "\n",
        "    # Finally, append the review the text!\n",
        "    combined += row['Long name']\n",
        "    \n",
        "    # Add the combined text to the list.\n",
        "    ln_w_dev.append(combined)\n",
        "\n",
        "    # Also record the sample's label.\n",
        "    new_labels.append(row['Code LN'])\n",
        "\n",
        "print('  DONE.')\n",
        "\n",
        "print('Dataset contains {:,} samples.'.format(len(ln_w_dev)))\n",
        "\n",
        "max_test_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for ln in ln_w_dev:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    new_input_ids = tokenizer.encode(ln, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_test_len = max(max_test_len, len(new_input_ids))\n",
        "\n",
        "print('New test Max sentence length: ', max_test_len)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "new_test_input_ids = []\n",
        "new_test_attention_masks = []\n",
        "\n",
        "print('Encoding all reviews in the dataset...')\n",
        "\n",
        "# For every sentence...\n",
        "for ln in ln_w_dev:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        ln,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_test_len,           # Pad & truncate all sentences.\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                )\n",
        "    # Add the encoded sentence to the list.    \n",
        "    new_test_input_ids.append(encoded_dict['input_ids'])\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    new_test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "#print(labels)\n",
        "new_test_input_ids = torch.cat(new_test_input_ids, dim=0)\n",
        "new_test_attention_masks = torch.cat(new_test_attention_masks, dim=0)\n",
        "new_labels = torch.tensor(new_labels)\n",
        "\n",
        "print(new_test_input_ids.shape)\n",
        "print(new_test_attention_masks.shape)\n",
        "print(new_labels.shape)\n",
        "\n",
        "print('DONE.')\n",
        "\n",
        "new_test_dataset = TensorDataset(new_test_input_ids[new_test_idx], new_test_attention_masks[new_test_idx], new_labels[new_test_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rGeoUP9PFmA",
        "outputId": "711e11a7-47df-41ad-c28f-3934f0e9faa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting labels for 701 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ],
      "source": [
        "# Create a DataLoader to batch our test samples for us. We'll use a sequential\n",
        "# sampler this time--don't need this to be random!\n",
        "prediction_sampler = SequentialSampler(test_dataset)\n",
        "prediction_dataloader = DataLoader(test_dataset, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "JPEq7XhLPJCk"
      },
      "outputs": [],
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUoLPAMYPMGD",
        "outputId": "c250a1c4-c20b-4471-cd8d-109be1938c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 average=micro Score: 0.966\n",
            "F1 average=macro Score: 0.921\n",
            "F1 average=weighted Score: 0.955\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1\n",
        "# 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "f1_micro = f1_score(flat_true_labels, flat_predictions, average='micro')\n",
        "\n",
        "print('F1 average=micro Score: %.3f' % f1_micro)\n",
        "\n",
        "# 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
        "f1_macro = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
        "\n",
        "print('F1 average=macro Score: %.3f' % f1_macro)\n",
        "\n",
        "# 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). \n",
        "# This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
        "f1_weighted = f1_score(flat_true_labels, flat_predictions, average='weighted')\n",
        "\n",
        "print('F1 average=weighted Score: %.3f' % f1_weighted)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
